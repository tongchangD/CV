# 注意力

## 通道上的注意力：SENet论文笔记

原文：Squeeze-and-Excitation Network [Cited by 4940]  
论文链接: https://https://arxiv.org/abs/1709.01507  
Caffe official code: https://github.com/hujie-frank/SENet  
pytorh high-star 复现：https://github.com/moskomule/senet.pytorch  

即插即用的通道注意力代码
```
class SELayer(nn.Module):
    def __init__(self, channel, reduction=16):
        super(SELayer, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.fc = nn.Sequential(
            nn.Linear(channel, channel // reduction, bias=False),
            nn.ReLU(inplace=True),
            nn.Linear(channel // reduction, channel, bias=False),
            nn.Sigmoid()
        )
    def forward(self, x):
        b, c, _, _ = x.size()
        y = self.avg_pool(x).view(b, c) # 对应压缩操作
        y = self.fc(y).view(b, c, 1, 1) # 对应Excitation操作
        return x * y.expand_as(x) # 把权重矩阵赋予到特征图
```
其中选择 **平均池化** 和 **Sigmoid**  比较优。  

