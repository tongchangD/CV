# 注意力

## 通道上的注意力：SENet论文笔记

原文：Squeeze-and-Excitation Network [Cited by 4940]  
论文链接: [地址](https://arxiv.org/abs/1709.01507)  
Caffe official [code](https://github.com/hujie-frank/SENet)  
pytorh high-star [复现](https://github.com/moskomule/senet.pytorch)  
博客介绍 [网址](https://blog.csdn.net/xys430381_1/article/details/89158063)

### 流程：  
- 将输入特征进行 Global AVE pooling，得到 1_1_ Channel  
- 然后bottleneck特征交互一下，先压缩 channel数，再重构回channel数  
- 最后接个 sigmoid，生成channel 间0~1的 attention weights，最后 scale 乘回原输入特征  

### 即插即用的通道注意力代码
```
class SELayer(nn.Module):
    def __init__(self, channel, reduction=16):
        super(SELayer, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.fc = nn.Sequential(
            nn.Linear(channel, channel // reduction, bias=False),
            nn.ReLU(inplace=True),
            nn.Linear(channel // reduction, channel, bias=False),
            nn.Sigmoid()
        )
    def forward(self, x):
        b, c, _, _ = x.size()
        y = self.avg_pool(x).view(b, c) # 对应压缩操作
        y = self.fc(y).view(b, c, 1, 1) # 对应Excitation操作
        return x * y.expand_as(x) # 把权重矩阵赋予到特征图
```
其中选择 **平均池化** 和 **Sigmoid**  比较优。  

